{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  任务4 模型选择(3天)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以lightGBM为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]\nLightGBM objective call #1 cur_best_score=0.00000\n  0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]Params: bagging_fraction=0.9435546300783195 bagging_freq=10.0 feature_fraction=0.7771956308960376 lambda_l1=5.814028290267602 lambda_l2=1.7829064960562069 learning_rate=0.00022924592626819452 max_bin=157.0 min_data_in_leaf=45.0 min_sum_hessian_in_leaf=1.6846085451212194 num_leaves=21.0\n  0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]aaa\n  0%|          | 0/500 [01:05<?, ?trial/s, best loss=?]10000\n  0%|          | 0/500 [01:05<?, ?trial/s, best loss=?]nb_trees=10000 val_loss={'rmse': 998.1039462497293}\n  0%|          | 0/500 [01:05<?, ?trial/s, best loss=?](40134,)\n  0%|          | 0/500 [01:08<?, ?trial/s, best loss=?](40134,)\n  0%|          | 0/500 [01:08<?, ?trial/s, best loss=?]aaa\n  0%|          | 0/500 [02:16<?, ?trial/s, best loss=?]10000\n  0%|          | 0/500 [02:16<?, ?trial/s, best loss=?]nb_trees=10000 val_loss={'rmse': 1029.144497703335}\n  0%|          | 0/500 [02:16<?, ?trial/s, best loss=?](40134,)\n  0%|          | 0/500 [02:19<?, ?trial/s, best loss=?](40134,)\n  0%|          | 0/500 [02:19<?, ?trial/s, best loss=?]aaa\n  0%|          | 0/500 [03:25<?, ?trial/s, best loss=?]10000\n  0%|          | 0/500 [03:25<?, ?trial/s, best loss=?]nb_trees=10000 val_loss={'rmse': 1035.7502924514447}\n  0%|          | 0/500 [03:25<?, ?trial/s, best loss=?](40134,)\n  0%|          | 0/500 [03:28<?, ?trial/s, best loss=?](40134,)\n  0%|          | 0/500 [03:28<?, ?trial/s, best loss=?]val_r2_score=0.7205046465664847\n  0%|          | 0/500 [03:28<?, ?trial/s, best loss=?]NEW BEST SCORE=0.7205046465664847\n  0%|          | 1/500 [03:28<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]\nLightGBM objective call #2 cur_best_score=0.72050\n  0%|          | 1/500 [03:28<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]Params: bagging_fraction=0.9238439413580236 bagging_freq=5.0 feature_fraction=0.7584319668570236 lambda_l1=7.210287616082458 lambda_l2=4.179942261312965 learning_rate=0.009806339865616394 max_bin=172.0 min_data_in_leaf=63.0 min_sum_hessian_in_leaf=6.161636766839361 num_leaves=24.0\n  0%|          | 1/500 [03:28<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]aaa\n  0%|          | 1/500 [04:01<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]6495\n  0%|          | 1/500 [04:01<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]nb_trees=6495 val_loss={'rmse': 723.0765565167234}\n  0%|          | 1/500 [04:01<28:53:51, 208.48s/trial, best loss: -0.7205046465664847](40134,)\n  0%|          | 1/500 [04:03<28:53:51, 208.48s/trial, best loss: -0.7205046465664847](40134,)\n  0%|          | 1/500 [04:03<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]aaa\n  0%|          | 1/500 [04:47<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]8483\n  0%|          | 1/500 [04:47<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]nb_trees=8483 val_loss={'rmse': 747.9378393700904}\n  0%|          | 1/500 [04:47<28:53:51, 208.48s/trial, best loss: -0.7205046465664847](40134,)\n  0%|          | 1/500 [04:50<28:53:51, 208.48s/trial, best loss: -0.7205046465664847](40134,)\n  0%|          | 1/500 [04:50<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]aaa\n  0%|          | 1/500 [05:23<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]6544\n  0%|          | 1/500 [05:23<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]nb_trees=6544 val_loss={'rmse': 756.6938462338421}\n  0%|          | 1/500 [05:23<28:53:51, 208.48s/trial, best loss: -0.7205046465664847](40134,)\n  0%|          | 1/500 [05:25<28:53:51, 208.48s/trial, best loss: -0.7205046465664847](40134,)\n  0%|          | 1/500 [05:25<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]val_r2_score=0.9025642533383493\n  0%|          | 1/500 [05:25<28:53:51, 208.48s/trial, best loss: -0.7205046465664847]NEW BEST SCORE=0.9025642533383493\n  0%|          | 2/500 [05:25<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]\nLightGBM objective call #3 cur_best_score=0.90256\n  0%|          | 2/500 [05:25<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]Params: bagging_fraction=0.8231301248251501 bagging_freq=3.0 feature_fraction=0.9388711423062334 lambda_l1=6.750620825557717 lambda_l2=6.498384150684099 learning_rate=0.0063821699910953525 max_bin=130.0 min_data_in_leaf=88.0 min_sum_hessian_in_leaf=1.7558039714376166 num_leaves=96.0\n  0%|          | 2/500 [05:25<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]aaa\n  0%|          | 2/500 [06:27<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]4401\n  0%|          | 2/500 [06:27<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]nb_trees=4401 val_loss={'rmse': 725.8757463625838}\n  0%|          | 2/500 [06:27<25:01:50, 180.95s/trial, best loss: -0.9025642533383493](40134,)\n  0%|          | 2/500 [06:30<25:01:50, 180.95s/trial, best loss: -0.9025642533383493](40134,)\n  0%|          | 2/500 [06:30<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]aaa\n  0%|          | 2/500 [07:33<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]4593\n  0%|          | 2/500 [07:33<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]nb_trees=4593 val_loss={'rmse': 752.2566738039312}\n  0%|          | 2/500 [07:33<25:01:50, 180.95s/trial, best loss: -0.9025642533383493](40134,)\n  0%|          | 2/500 [07:37<25:01:50, 180.95s/trial, best loss: -0.9025642533383493](40134,)\n  0%|          | 2/500 [07:37<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]aaa\n  0%|          | 2/500 [08:26<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]3534\n  0%|          | 2/500 [08:26<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]nb_trees=3534 val_loss={'rmse': 754.3571206702604}\n  0%|          | 2/500 [08:26<25:01:50, 180.95s/trial, best loss: -0.9025642533383493](40134,)\n  0%|          | 2/500 [08:28<25:01:50, 180.95s/trial, best loss: -0.9025642533383493](40134,)\n  0%|          | 2/500 [08:28<25:01:50, 180.95s/trial, best loss: -0.9025642533383493]val_r2_score=0.902242200087104\n  1%|          | 3/500 [08:28<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]\nLightGBM objective call #4 cur_best_score=0.90256\n  1%|          | 3/500 [08:28<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]Params: bagging_fraction=0.9666195108370552 bagging_freq=7.0 feature_fraction=0.9252033685851179 lambda_l1=0.3673134035322023 lambda_l2=1.053101833961655 learning_rate=0.006274986174682755 max_bin=125.0 min_data_in_leaf=34.0 min_sum_hessian_in_leaf=2.378096751525009 num_leaves=74.0\n  1%|          | 3/500 [08:28<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]aaa\n  1%|          | 3/500 [09:18<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]4489\n  1%|          | 3/500 [09:18<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]nb_trees=4489 val_loss={'rmse': 722.9464215907573}\n  1%|          | 3/500 [09:18<25:05:31, 181.75s/trial, best loss: -0.9025642533383493](40134,)\n  1%|          | 3/500 [09:20<25:05:31, 181.75s/trial, best loss: -0.9025642533383493](40134,)\n  1%|          | 3/500 [09:20<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]aaa\n  1%|          | 3/500 [10:10<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]4402\n  1%|          | 3/500 [10:10<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]nb_trees=4402 val_loss={'rmse': 748.3908266663979}\n  1%|          | 3/500 [10:10<25:05:31, 181.75s/trial, best loss: -0.9025642533383493](40134,)\n  1%|          | 3/500 [10:12<25:05:31, 181.75s/trial, best loss: -0.9025642533383493](40134,)\n  1%|          | 3/500 [10:12<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]aaa\n  1%|          | 3/500 [10:55<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]3871\n  1%|          | 3/500 [10:55<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]nb_trees=3871 val_loss={'rmse': 752.4571537868395}\n  1%|          | 3/500 [10:55<25:05:31, 181.75s/trial, best loss: -0.9025642533383493](40134,)\n  1%|          | 3/500 [10:56<25:05:31, 181.75s/trial, best loss: -0.9025642533383493](40134,)\n  1%|          | 3/500 [10:56<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]val_r2_score=0.9027023086384318\n  1%|          | 3/500 [10:57<25:05:31, 181.75s/trial, best loss: -0.9025642533383493]NEW BEST SCORE=0.9027023086384318\n  1%|          | 4/500 [10:57<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]\nLightGBM objective call #5 cur_best_score=0.90270\n  1%|          | 4/500 [10:57<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]Params: bagging_fraction=0.9039185062761876 bagging_freq=7.0 feature_fraction=0.9035236427711927 lambda_l1=9.64647544806503 lambda_l2=7.452483159861389 learning_rate=0.0009391888949633653 max_bin=156.0 min_data_in_leaf=93.0 min_sum_hessian_in_leaf=7.861447945380211 num_leaves=21.0\n  1%|          | 4/500 [10:57<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]aaa\n  1%|          | 4/500 [11:58<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]10000\n  1%|          | 4/500 [11:58<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]nb_trees=10000 val_loss={'rmse': 799.7271798020499}\n  1%|          | 4/500 [11:58<23:39:23, 171.70s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 4/500 [12:01<23:39:23, 171.70s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 4/500 [12:01<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]aaa\n  1%|          | 4/500 [13:05<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]10000\n  1%|          | 4/500 [13:05<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]nb_trees=10000 val_loss={'rmse': 826.0019262021823}\n  1%|          | 4/500 [13:05<23:39:23, 171.70s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 4/500 [13:08<23:39:23, 171.70s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 4/500 [13:08<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]aaa\n  1%|          | 4/500 [14:09<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]10000\n  1%|          | 4/500 [14:09<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]nb_trees=10000 val_loss={'rmse': 826.1596809021127}\n  1%|          | 4/500 [14:09<23:39:23, 171.70s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 4/500 [14:12<23:39:23, 171.70s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 4/500 [14:12<23:39:23, 171.70s/trial, best loss: -0.9027023086384318]val_r2_score=0.8762169900247976\n  1%|          | 5/500 [14:12<24:35:49, 178.89s/trial, best loss: -0.9027023086384318]\nLightGBM objective call #6 cur_best_score=0.90270\n  1%|          | 5/500 [14:12<24:35:49, 178.89s/trial, best loss: -0.9027023086384318]Params: bagging_fraction=0.7509725127497068 bagging_freq=7.0 feature_fraction=0.9718225840740188 lambda_l1=9.522220381375927 lambda_l2=8.36977036356075 learning_rate=0.0075173071793206885 max_bin=155.0 min_data_in_leaf=51.0 min_sum_hessian_in_leaf=3.421208100607507 num_leaves=38.0\n  1%|          | 5/500 [14:12<24:35:49, 178.89s/trial, best loss: -0.9027023086384318]aaa\n  1%|          | 5/500 [15:00<24:35:49, 178.89s/trial, best loss: -0.9027023086384318]6489\n  1%|          | 5/500 [15:00<24:35:49, 178.89s/trial, best loss: -0.9027023086384318]nb_trees=6489 val_loss={'rmse': 725.1470509012346}\n  1%|          | 5/500 [15:00<24:35:49, 178.89s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 5/500 [15:02<24:35:49, 178.89s/trial, best loss: -0.9027023086384318](40134,)\n  1%|          | 5/500 [15:02<24:35:49, 178.89s/trial, best loss: -0.9027023086384318]"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import numpy\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import colorama\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "N_HYPEROPT_PROBES = 500\n",
    "HYPEROPT_ALGO = tpe.suggest  #  tpe.suggest OR hyperopt.rand.suggest\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "colorama.init()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def get_lgb_params(space):\n",
    "    lgb_params = dict()\n",
    "    lgb_params['boosting_type'] = space['boosting_type'] if 'boosting_type' in space else 'gbdt'\n",
    "    lgb_params['objective'] = 'regression'\n",
    "    lgb_params['metric'] = 'rmse'\n",
    "    lgb_params['learning_rate'] = space['learning_rate']\n",
    "    lgb_params['num_leaves'] = int(space['num_leaves'])\n",
    "    lgb_params['min_data_in_leaf'] = int(space['min_data_in_leaf'])\n",
    "    lgb_params['min_sum_hessian_in_leaf'] = space['min_sum_hessian_in_leaf']\n",
    "    lgb_params['max_depth'] = -1\n",
    "    lgb_params['lambda_l1'] = space['lambda_l1'] if 'lambda_l1' in space else 0.0\n",
    "    lgb_params['lambda_l2'] = space['lambda_l2'] if 'lambda_l2' in space else 0.0\n",
    "    lgb_params['max_bin'] = int(space['max_bin']) if 'max_bin' in space else 256\n",
    "    lgb_params['feature_fraction'] = space['feature_fraction']\n",
    "    lgb_params['bagging_fraction'] = space['bagging_fraction']\n",
    "    lgb_params['bagging_freq'] = int(space['bagging_freq']) if 'bagging_freq' in space else 1\n",
    "    lgb_params['nthread'] = 4\n",
    "    return lgb_params\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "obj_call_count = 0\n",
    "cur_best_score = 0 # 0 or np.inf\n",
    "log_writer = open( 'log/lgb-hyperopt-log.txt', 'w' )\n",
    "X_train = pd.read_csv('../trainx.csv')\n",
    "X_test = pd.read_csv('../testx.csv')\n",
    "Y_train = pd.read_csv('../trainy.csv', header=None)\n",
    "Y_test = pd.read_csv('../testy.csv', header=None)\n",
    "\n",
    "def objective(space):\n",
    "    global obj_call_count, cur_best_score\n",
    "\n",
    "    obj_call_count += 1\n",
    "\n",
    "    print('\\nLightGBM objective call #{} cur_best_score={:7.5f}'.format(obj_call_count,cur_best_score) )\n",
    "\n",
    "    lgb_params = get_lgb_params(space)\n",
    "\n",
    "    sorted_params = sorted(space.items(), key=lambda z: z[0])\n",
    "    params_str = str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params])\n",
    "    print('Params: {}'.format(params_str) )\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    out_of_fold = np.zeros(len(X_train))\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        D_train = lgb.Dataset(X_train.iloc[train_idx], label=Y_train.iloc[train_idx])\n",
    "        # D_train = lgb.Dataset(X_train.iloc[train_idx].values, label=Y_train.iloc[train_idx].tolist())\n",
    "        D_val = lgb.Dataset(X_train.iloc[val_idx], label=Y_train.iloc[val_idx])\n",
    "        # Train\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(lgb_params,\n",
    "                           D_train,\n",
    "                           num_boost_round=num_round,\n",
    "                           # metrics='mlogloss',\n",
    "                           valid_sets=D_val,\n",
    "                           # valid_names='val',\n",
    "                           # fobj=None,\n",
    "                           # feval=None,\n",
    "                           # init_model=None,\n",
    "                           # feature_name='auto',\n",
    "                           # categorical_feature='auto',\n",
    "                           early_stopping_rounds=200,\n",
    "                           # evals_result=None,\n",
    "                           verbose_eval=False,\n",
    "                           # learning_rates=None,\n",
    "                           # keep_training_booster=False,\n",
    "                           # callbacks=None\n",
    "                           )\n",
    "        # predict\n",
    "        nb_trees = clf.best_iteration\n",
    "        print(\"aaa\", nb_trees)\n",
    "        val_loss = clf.best_score['valid_0']\n",
    "        print('nb_trees={} val_loss={}'.format(nb_trees, val_loss))\n",
    "        out_of_fold[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=nb_trees)\n",
    "        score = r2_score(out_of_fold, Y_train)\n",
    "        # score = r2_score(out_of_fold, Y_train.values.reshape(-1))\n",
    "\n",
    "    print('val_r2_score={}'.format(score))\n",
    "\n",
    "    log_writer.write('score={} Params:{} nb_trees={}\\n'.format(score, params_str, nb_trees ))\n",
    "    log_writer.flush()\n",
    "\n",
    "    if score>cur_best_score:\n",
    "        cur_best_score = score\n",
    "        print(colorama.Fore.GREEN + 'NEW BEST SCORE={}'.format(cur_best_score) + colorama.Fore.RESET)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "space ={\n",
    "        'num_leaves': hp.quniform ('num_leaves', 10, 100, 1),\n",
    "        'min_data_in_leaf':  hp.quniform ('min_data_in_leaf', 10, 100, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.75, 1.0),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.75, 1.0),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0, 0.01),\n",
    "#         'learning_rate': hp.loguniform('learning_rate', -5.0, -2.3),\n",
    "        'min_sum_hessian_in_leaf': hp.loguniform('min_sum_hessian_in_leaf', 0, 2.3),\n",
    "        'max_bin': hp.quniform ('max_bin', 88, 200, 1),\n",
    "        'bagging_freq': hp.quniform ('bagging_freq', 1, 15, 1),\n",
    "        'lambda_l1': hp.uniform('lambda_l1', 0, 10 ),\n",
    "        'lambda_l2': hp.uniform('lambda_l2', 0, 10 ),\n",
    "       }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=HYPEROPT_ALGO,\n",
    "                max_evals=N_HYPEROPT_PROBES,\n",
    "                trials=trials,\n",
    "                verbose=1)\n",
    "\n",
    "print('-'*50)\n",
    "print('The best params:')\n",
    "print( best )\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[    0     1     2 ... 40129 40130 40131][    4     6     7 ... 40127 40132 40133]\n[    4     5     6 ... 40131 40132 40133][    0     1     2 ... 40126 40129 40130]\n[    0     1     2 ... 40130 40132 40133][    5    10    11 ... 40122 40128 40131]\n"
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "out_of_fold = np.zeros(len(X_train))\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(train_idx, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}